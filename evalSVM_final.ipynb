{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import cv2\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.linear_model import Lasso\n",
    "# from scikeras.wrappers import KerasClassifier, KerasRegressor\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "landmarksJaffe shape: (213, 207)\n",
      "landmarksKdef.shape: (2917, 207)\n",
      "landmarksCKplus.shape: (840, 207)\n",
      "landmarksCustom.shape: (118, 207)\n",
      "gaborPCA shape: (4088, 40)\n"
     ]
    }
   ],
   "source": [
    "readDataPath = './resources/'\n",
    "# Read datasets\n",
    "landmarksJaffe = pd.read_csv(readDataPath+'features/landmarksJaffe.csv')\n",
    "print(f\"landmarksJaffe shape: {landmarksJaffe.shape}\")\n",
    "landmarksKdef = pd.read_csv(readDataPath+'features/landmarksKDEF.csv')\n",
    "print(f\"landmarksKdef.shape: {landmarksKdef.shape}\")\n",
    "landmarksCKplus = pd.read_csv(readDataPath+'features/landmarksCKPLUS.csv')\n",
    "print(f\"landmarksCKplus.shape: {landmarksCKplus.shape}\")\n",
    "landmarksCustom = pd.read_csv(readDataPath+'features/landmarksCustom.csv')\n",
    "print(f\"landmarksCustom.shape: {landmarksCustom.shape}\")\n",
    "gaborPCA = pd.read_csv(readDataPath+'features/gaborPCA.csv')\n",
    "print(f\"gaborPCA shape: {gaborPCA.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenate 3D landmark datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emotionLandmarks shape: (4088, 207)\n"
     ]
    }
   ],
   "source": [
    "# stack datasets\n",
    "emotionLandmarks = pd.concat([landmarksJaffe, landmarksKdef, landmarksCKplus, landmarksCustom], axis=0)\n",
    "print(f\"emotionLandmarks shape: {emotionLandmarks.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split 3D landmark Training data & Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_landmarks shape: (4088, 206)\n"
     ]
    }
   ],
   "source": [
    "X_landmarks = emotionLandmarks.drop('emotion', axis=1)\n",
    "y = emotionLandmarks['emotion'].astype('int8')\n",
    "print(f\"X_landmarks shape: {X_landmarks.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling 3D landmarks & Gabor datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_3D_scaled Shape = (4088, 206)\n",
      "gaborPCA_scaled Shape = (4088, 40)\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "# 3D landmark scaling\n",
    "X_landmarks_scaled = scaler.fit_transform(X_landmarks)\n",
    "print(f\"X_3D_scaled Shape = {np.shape(X_landmarks_scaled)}\")\n",
    "# print(f\"scaler min = {scaler.data_min_}, scaler max = {scaler.data_max_}\")\n",
    "\n",
    "# # save min/max values of each feature\n",
    "# with open('./datasets/combiModels/scalerParams_svm_pp.pkl', 'wb') as file:\n",
    "#     params = {'minValues': scaler.data_min_, 'maxValues': scaler.data_max_}\n",
    "#     pickle.dump(params, file)\n",
    "# file.close()\n",
    "\n",
    "# Gabor scaling\n",
    "gaborPCA_scaled = scaler.fit_transform(gaborPCA)\n",
    "print(f\"gaborPCA_scaled Shape = {np.shape(gaborPCA_scaled)}\")\n",
    "# # save min/max values of each feature\n",
    "# with open('./datasets/combiModels/scalerParamsGabor_svm_pp.pkl', 'wb') as file:\n",
    "#     params = {'minValues': scaler.data_min_, 'maxValues': scaler.data_max_}\n",
    "#     pickle.dump(params, file)\n",
    "# file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction of 3D landmark dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_3D_scaled_pca shape: (4088, 120), type: <class 'numpy.ndarray'>\n",
      "Explained variance: 99.3%\n"
     ]
    }
   ],
   "source": [
    "n_components = 120\n",
    "pca = PCA(n_components= n_components)\n",
    "X_landmarks_scaled_pca = pca.fit_transform(X_landmarks_scaled)\n",
    "print(f\"X_3D_scaled_pca shape: {X_landmarks_scaled_pca.shape}, type: {type(X_landmarks_scaled_pca)}\")\n",
    "\n",
    "# # Save the fitted PCA model to a file\n",
    "# with open(readDataPath+'combiModels/pca_svm_pp.pkl', 'wb') as file:\n",
    "#     pickle.dump(pca, file)\n",
    "# file.close()\n",
    "\n",
    "print(f\"Explained variance: {round(np.cumsum(pca.explained_variance_ratio_ * 100)[-1],2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenate & Shuffle datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_landmarks_scaled_pca shape: (4088, 120), type: <class 'numpy.ndarray'>\n",
      "gaborPCA_scaled shape: (4088, 40), type: <class 'numpy.ndarray'>\n",
      "y shape: (4088,), type: <class 'pandas.core.series.Series'>\n",
      "X_landmarks_gabor_scaled_pca_shuffled shape: (4088, 160), type: <class 'numpy.ndarray'>\n",
      "y shape: (4088,), type: <class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "print(f\"X_landmarks_scaled_pca shape: {X_landmarks_scaled_pca.shape}, type: {type(X_landmarks_scaled_pca)}\")\n",
    "print(f\"gaborPCA_scaled shape: {gaborPCA_scaled.shape}, type: {type(gaborPCA_scaled)}\")\n",
    "print(f\"y shape: {y.shape}, type: {type(y)}\")\n",
    "\n",
    "# concatenate landmark and gabor datasets\n",
    "X_landmarks_gabor_scaled_pca = np.concatenate((X_landmarks_scaled_pca, gaborPCA_scaled), axis=1)\n",
    "\n",
    "# shuffle\n",
    "X_landmarks_gabor_scaled_pca_shuffled, y_shuffled = shuffle(X_landmarks_gabor_scaled_pca, y, random_state=512)\n",
    "\n",
    "print(f\"X_landmarks_gabor_scaled_pca_shuffled shape: {X_landmarks_gabor_scaled_pca_shuffled.shape}, type: {type(X_landmarks_gabor_scaled_pca_shuffled)}\")\n",
    "print(f\"y shape: {y_shuffled.shape}, type: {type(y_shuffled)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check label count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "angry count = 569\n",
      "disgust count = 602\n",
      "fear count = 538\n",
      "happy count = 654\n",
      "sad count = 518\n",
      "surprised count = 664\n",
      "neutral count = 543\n"
     ]
    }
   ],
   "source": [
    "# string labels\n",
    "emo = ['angry', \n",
    "        'disgust',\n",
    "        'fear',\n",
    "        'happy',\n",
    "        'sad',\n",
    "        'surprised',\n",
    "        'neutral'\n",
    "]\n",
    "\n",
    "# count classes\n",
    "emotions, counts =np.unique(y_shuffled, return_counts=True)\n",
    "for c, count in zip(emotions, counts):\n",
    "    print(f\"{emo[c]} count = {count}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build and Validate Single run SVM-RBF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 100.0%, Validation accuracy: 89.24%\n"
     ]
    }
   ],
   "source": [
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_landmarks_gabor_scaled_pca_shuffled, y_shuffled, test_size=0.1, shuffle=True, random_state=190)\n",
    "\n",
    "# train model\n",
    "svmClf = SVC(kernel ='rbf', C=7.5, gamma=0.35)\n",
    "hist = svmClf.fit(X_train, y_train)\n",
    "\n",
    "# Calculate training accuracy\n",
    "y_train_pred = svmClf.predict(X_train)\n",
    "train_acc = accuracy_score(y_train, y_train_pred)\n",
    "\n",
    "# Calculate validation accuracy\n",
    "y_val_pred = svmClf.predict(X_test)\n",
    "val_acc = accuracy_score(y_test, y_val_pred)\n",
    "\n",
    "print(f\"Training accuracy: {round(train_acc*100,2)}%, Validation accuracy: {round(val_acc*100,2)}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix Typical Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Test Instances= 409\n",
      "Accuracy= 89.24%\n",
      "Confusion Matrix\n",
      "[[45  3  1  0  1  0  1]\n",
      " [ 3 43  2  0  1  0  0]\n",
      " [ 2  1 43  1  0  9  0]\n",
      " [ 0  0  0 64  0  0  0]\n",
      " [ 2  2  2  0 49  0  4]\n",
      " [ 0  0  3  1  1 64  0]\n",
      " [ 1  0  1  0  2  0 57]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = svmClf.predict(X_test)\n",
    "cm = confusion_matrix(y_true=y_test, y_pred=y_pred)\n",
    "TP = cm.diagonal().sum()\n",
    "totalIns = cm.sum()\n",
    "\n",
    "print(f\"Number of Test Instances= {totalIns}\")\n",
    "print(f\"Accuracy= {round((TP/totalIns)*100,2)}%\")\n",
    "print('Confusion Matrix')\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search Best Estimator for SVM-RBF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 2 candidates, totalling 20 fits\n",
      "[CV] END .........................svm__C=7.5, svm__gamma=0.3; total time=   3.7s\n",
      "[CV] END .........................svm__C=7.5, svm__gamma=0.3; total time=   3.7s\n",
      "[CV] END .........................svm__C=7.5, svm__gamma=0.3; total time=   3.7s\n",
      "[CV] END .........................svm__C=7.5, svm__gamma=0.3; total time=   3.7s\n",
      "[CV] END .........................svm__C=7.5, svm__gamma=0.3; total time=   3.7s\n",
      "[CV] END .........................svm__C=7.5, svm__gamma=0.3; total time=   3.7s\n",
      "[CV] END .........................svm__C=7.5, svm__gamma=0.3; total time=   3.7s\n",
      "[CV] END .........................svm__C=7.5, svm__gamma=0.3; total time=   3.9s\n",
      "[CV] END .........................svm__C=7.5, svm__gamma=0.3; total time=   3.8s\n",
      "[CV] END .........................svm__C=7.5, svm__gamma=0.3; total time=   3.7s\n",
      "[CV] END ........................svm__C=7.5, svm__gamma=0.35; total time=   4.2s\n",
      "[CV] END ........................svm__C=7.5, svm__gamma=0.35; total time=   4.1s\n",
      "[CV] END ........................svm__C=7.5, svm__gamma=0.35; total time=   4.1s\n",
      "[CV] END ........................svm__C=7.5, svm__gamma=0.35; total time=   4.2s\n",
      "[CV] END ........................svm__C=7.5, svm__gamma=0.35; total time=   4.2s\n",
      "[CV] END ........................svm__C=7.5, svm__gamma=0.35; total time=   4.2s\n",
      "[CV] END ........................svm__C=7.5, svm__gamma=0.35; total time=   4.1s\n",
      "[CV] END ........................svm__C=7.5, svm__gamma=0.35; total time=   4.2s\n",
      "[CV] END ........................svm__C=7.5, svm__gamma=0.35; total time=   4.2s\n",
      "[CV] END ........................svm__C=7.5, svm__gamma=0.35; total time=   4.2s\n",
      "Best hyperparameters: {'svm__C': 7.5, 'svm__gamma': 0.35}\n",
      "Best score: 85.92999999999999%\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "                        ('svm', SVC(kernel ='rbf', probability=True, random_state=42)),\n",
    "])\n",
    "\n",
    "# define the hyperparameters to search over\n",
    "param_grid = {\n",
    "    'svm__C': [7.5],\n",
    "    'svm__gamma': [0.3,0.35],\n",
    "}\n",
    "\n",
    "# perform a grid search to find the best hyperparameters\n",
    "grid_search = GridSearchCV(pipeline, param_grid=param_grid, cv=10, verbose=2)\n",
    "hist = grid_search.fit(X_landmarks_gabor_scaled_pca_shuffled, y_shuffled)\n",
    "\n",
    "# print the best hyperparameters and the corresponding score\n",
    "print(f\"Best hyperparameters: {grid_search.best_params_}\")\n",
    "print(f\"Best score: {round(grid_search.best_score_,4)*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Cross-Validation Score: 85.93%\n"
     ]
    }
   ],
   "source": [
    "k=10\n",
    "svmClfCV = SVC(kernel ='rbf', C=7.5, gamma=0.35)\n",
    "scores = cross_val_score(svmClfCV, X_landmarks_gabor_scaled_pca_shuffled, y_shuffled, cv=k)\n",
    "print(f\"Mean Cross-Validation Score: {round(np.mean(scores)*100,2)}%\") #84.59"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Serialized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bestModel = grid_search.best_estimator_\n",
    "# with open(readDataPath+'combiModels/modelSvmRbf_pp.pkl', 'wb') as file:\n",
    "#     pickle.dump(bestModel,file)\n",
    "\n",
    "# file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3916v4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
